{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+oPLft6B8B0QT6ITpqGWf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wayne-wyyking888/Stat-8931-GenAI/blob/main/chapter3/Chapter_III_Training_LLMs_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reasons of training LLM from scratch\n",
        "* **Domain-Specificity**: Training with industry-specific data.\n",
        "\n",
        "* **Greater Data Security**: Incorporating sensitive or proprietary information securely.\n",
        "\n",
        "* **Ownership and Control**: Retaining control over confidential data and improving the model over time."
      ],
      "metadata": {
        "id": "ObwHqDbmyhXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transformer Model Architecture**"
      ],
      "metadata": {
        "id": "cTeLGzf4zs4L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYtTx6aStB23"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.vocab_size = args.vocab_size\n",
        "\n",
        "        # Convert input token indices into dense vector representations\n",
        "        self.tok_embeddings = nn.Embedding(args.vocab_size, args.dim)\n",
        "\n",
        "        # Add transformer blocks here\n",
        "        ...\n",
        "\n",
        "        # Convert the final hidden state of the model back into a distribution over the vocabulary\n",
        "        self.output = nn.Linear(args.dim, args.vocab_size, bias=False)\n",
        "\n",
        "        # Weight Tying: using the same weight matrix to reduce complexity\n",
        "        self.tok_embeddings.weight = self.output.weight\n",
        "\n",
        "        # Precompute positional embeddings\n",
        "        self.freqs_cos, self.freqs_sin = ...\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor, targets: Optional[torch.Tensor] = None):\n",
        "        h = self.tok_embeddings(tokens)\n",
        "        h = self.dropout(h)\n",
        "        for layer in self.layers:\n",
        "            h = layer(h, self.freqs_cos[:seqlen], self.freqs_sin[:seqlen])\n",
        "        h = self.norm(h)\n",
        "\n",
        "        if targets is not None: # training-stage\n",
        "            logits = self.output(h)\n",
        "            self.last_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else: # inference-stage: only select the hidden state of the last token in each sequence\n",
        "            logits = self.output(h[:, [-1], :])\n",
        "            self.last_loss = None\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Llama 7B ModelArgs"
      ],
      "metadata": {
        "id": "VNEPTEv50UqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "@dataclass\n",
        "class ModelArgs:\n",
        "    dim: int = 4096\n",
        "    n_layers: int = 32\n",
        "    n_heads: int = 32\n",
        "    n_kv_heads: Optional[int] = None\n",
        "    vocab_size: int = 32000\n",
        "    hidden_dim: Optional[int] = None\n",
        "    multiple_of: int = 256\n",
        "    norm_eps: float = 1e-5\n",
        "    max_seq_len: int = 2048\n",
        "    dropout: float = 0.0"
      ],
      "metadata": {
        "id": "ZdUz2jlc0aZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Transformer Blocks"
      ],
      "metadata": {
        "id": "YHtALt1q1D6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        ...\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        for layer_id in range(args.n_layers):\n",
        "            self.layers.append(TransformerBlock(layer_id, args))\n",
        "\n",
        "        # Normalizes the input to the attention and feed-forward layers.\n",
        "        self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "        ...\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, layer_id: int, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.n_heads = args.n_heads\n",
        "        self.dim = args.dim\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "        self.attention = Attention(args)\n",
        "        self.feed_forward = FeedForward(dim=args.dim, hidden_dim=args.hidden_dim, multiple_of=args.multiple_of, dropout=args.dropout)\n",
        "        self.layer_id = layer_id\n",
        "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "\n",
        "    def forward(self, x, freqs_cos, freqs_sin):\n",
        "        h = x + self.attention.forward(self.attention_norm(x), freqs_cos, freqs_sin)\n",
        "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
        "        return out"
      ],
      "metadata": {
        "id": "l8eO9-1X1L4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Self-Attention Blocks"
      ],
      "metadata": {
        "id": "Y4IysXXw4KQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "\n",
        "        # QKV projections\n",
        "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
        "        self.wk = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
        "        self.wv = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
        "\n",
        "        # Final projection into the residual stream\n",
        "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
        "\n",
        "        # Create a mask for causal attention\n",
        "        mask = torch.full((1, 1, args.max_seq_len, args.max_seq_len), float(\"-inf\"))\n",
        "        self.mask = torch.triu(mask, diagonal=1)\n",
        "        ...\n",
        "\n",
        "    def forward(self, x: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):\n",
        "\n",
        "        # reorganize dimensions and apply relative positional embeddings to update xq, xk using freqs_cos, freqs_sin\n",
        "        ...\n",
        "\n",
        "        scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "        scores = scores + self.mask[:, :, :seqlen, :seqlen]\n",
        "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
        "        output = torch.matmul(scores, xv)\n",
        "        output = self.wo(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "l_WyPhjJ4N-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Types\n",
        "1. Decoder-only models use unidirectional attention to predict the next token in a sequence\n",
        "\n",
        "2. Encoder-only focuses on bidirectional context\n",
        "\n",
        "3. Encoder-decoder uses a decoder that attends to both the input (through encoder-decoder attention) and its own past outputs (through self-attention)"
      ],
      "metadata": {
        "id": "r8V6p1uEDtH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Existing Public Datasets**: Data that has been previously used to train LLMs and made available for public use. Prominent examples include:\n",
        "\n",
        "* The Common Crawl: A dataset containing terabytes of raw web data extracted from billions of pages\n",
        "\n",
        "* The Pile: A dataset that contains data from 22 data sources across 5 categories:\n",
        "\n",
        "    Academic Writing: e.g., arXiv\n",
        "\n",
        "    Online or Scraped Resources: e.g., Wikipedia\n",
        "\n",
        "    Prose: e.g., Project Gutenberg\n",
        "\n",
        "    Dialog: e.g., YouTube subtitles\n",
        "\n",
        "    Miscellaneous: e.g., GitHub\n",
        "\n",
        "* StarCoder: Near 800GB of coding samples in several programming languages\n",
        "\n",
        "* Hugging Face: An online community with over 100,000 public datasets\n",
        "\n",
        "* Private Datasets: Curated in-house or purchased\n",
        "\n",
        "* Directly From the Internet: Less recommended due to potential inaccuracies/biases"
      ],
      "metadata": {
        "id": "CekT-xcAEJRr"
      }
    }
  ]
}