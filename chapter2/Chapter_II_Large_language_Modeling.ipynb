{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkKBD3tdEKleNx9GMM11zW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wayne-wyyking888/Stat-8931-GenAI/blob/main/chapter2/Chapter_II_Large_language_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages, Paths & Environment"
      ],
      "metadata": {
        "id": "Fxm6kceGpwQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1). Download the dependent .py files; trained model; tokenizer to the **currect directory**"
      ],
      "metadata": {
        "id": "HaIDtMBLtWy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download model.py and tokenizer.py\n",
        "! gdown --id 1SU7jSZI36KGwBv5-zgc3WkStPK6lGKwL -O /content/model.py # download model.py\n",
        "! gdown --id 1uXCgdmip79J6efM5hiHGCy9mdr_U8BXT -O /content/tokenizer.py # download tokenizer.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvVP5B0x23TJ",
        "outputId": "f34b35b7-baeb-4c94-9215-01f65d1088dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1SU7jSZI36KGwBv5-zgc3WkStPK6lGKwL\n",
            "From (redirected): https://drive.google.com/uc?id=1SU7jSZI36KGwBv5-zgc3WkStPK6lGKwL&confirm=t&uuid=e1de7aff-babd-4ab8-a628-d104d4cab4da\n",
            "To: /content/model.py\n",
            "100% 13.3k/13.3k [00:00<00:00, 36.5MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1uXCgdmip79J6efM5hiHGCy9mdr_U8BXT\n",
            "From (redirected): https://drive.google.com/uc?id=1uXCgdmip79J6efM5hiHGCy9mdr_U8BXT&confirm=t&uuid=a4e0a780-ba5b-44c9-95b1-16475e49bef0\n",
            "To: /content/tokenizer.py\n",
            "100% 1.35k/1.35k [00:00<00:00, 4.28MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHiS9pHfpS4V",
        "outputId": "9fdba2d8-05eb-4686-f90d-60d0436f82e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/models’: File exists\n",
            "mkdir: cannot create directory ‘/content/data’: File exists\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1npTlkOP_TtW-oEMROTDrYXzOjLL2yuUQ\n",
            "From (redirected): https://drive.google.com/uc?id=1npTlkOP_TtW-oEMROTDrYXzOjLL2yuUQ&confirm=t&uuid=9e157553-b01b-409c-bb04-8c95d308aa2b\n",
            "To: /content/models/trained_model.pth\n",
            "100% 104M/104M [00:00<00:00, 136MB/s] \n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LezVTflV_Irp0GQn0Rzgn3F77tppozGa\n",
            "To: /content/data/trained_tokenizer.model\n",
            "100% 500k/500k [00:00<00:00, 100MB/s]\n"
          ]
        }
      ],
      "source": [
        "# trained language model (checkpoints) and tokenizer files under suitable directories\n",
        "# (1) make directories\n",
        "! mkdir /content/models\n",
        "! mkdir /content/data\n",
        "# (2) download checkpoint and tokenizers\n",
        "! gdown --id 1npTlkOP_TtW-oEMROTDrYXzOjLL2yuUQ -O /content/models/trained_model.pt\n",
        "! gdown --id 1LezVTflV_Irp0GQn0Rzgn3F77tppozGa -O /content/data/trained_tokenizer.model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2) Load pretrained model and test\n",
        "* Load pretrained model\n",
        "* Adjust TF32 precision\n",
        "* Config parameters for generation & decoding"
      ],
      "metadata": {
        "id": "RaX8l8O36AiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from contextlib import nullcontext\n",
        "import torch\n",
        "from model import ModelArgs, Transformer\n",
        "from tokenizer import Tokenizer\n",
        "import os\n",
        "\n",
        "checkpoint = os.getcwd() + \"/trained_model.pt\"\n",
        "tokenizer = os.getcwd() + \"/data/trained_tokenizer.model\"\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Use device: {device}\")\n",
        "\n",
        "# load checkpoint\n",
        "checkpoint_dict = torch.load(checkpoint, map_location=device)\n",
        "gptconf = ModelArgs(**checkpoint_dict['model_args'])\n",
        "model = Transformer(gptconf)\n",
        "state_dict = checkpoint_dict['model']\n",
        "unwanted_prefix = '_orig_mod.' #the unwanted prefix was sometimes added during compiling\n",
        "for k,v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# load tokenizer\n",
        "enc = Tokenizer(tokenizer_model=tokenizer)\n",
        "\n",
        "# adjust precision\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "torch.backends.cuda.matmul.allow_tf32 = True #enables the use of TF32 for matrix multiplication operations within PyTorch when using CUDA\n",
        "torch.backends.cudnn.allow_tf32 = True #enables the use of TF32 precision within the cuDNN library\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "9fJzX3XSyAP3",
        "outputId": "0e0969be-84b2-46b3-b15a-bce38ff40319"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-42fd5ddb1fe9>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint_dict = torch.load(checkpoint, map_location=device)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-42fd5ddb1fe9>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# load checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mcheckpoint_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mgptconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcheckpoint_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_args'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgptconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0morig_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0moverall_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_is_torchscript_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m                     warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! stat /content/trained_model.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKVoW4qCBZ9U",
        "outputId": "cb51eadd-9297-4c5e-b631-715c7e957262"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  File: /content/trained_model.pt\n",
            "  Size: 88080384  \tBlocks: 172040     IO Block: 4096   regular file\n",
            "Device: 2fh/47d\tInode: 4980878     Links: 1\n",
            "Access: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)\n",
            "Access: 2024-09-11 21:38:15.371371658 +0000\n",
            "Modify: 2024-09-11 21:36:51.963609463 +0000\n",
            "Change: 2024-09-11 21:36:52.005612868 +0000\n",
            " Birth: 2024-09-11 21:36:28.899739586 +0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stat $checkpoint"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "HEjZucJy9S8T",
        "outputId": "659dcf18-0ed0-4c4a-8711-d64d304c0123"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-6-18a79ac15844>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-18a79ac15844>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    stat $checkpoint\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## parameter configurations\n",
        "num_samples = 1 # number of samples to draw\n",
        "max_new_tokens = 256 # number of tokens generated in each sample\n",
        "temperature = 1.0 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 300 # retain only the top_k most likely tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "8i806WDT2orY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate texts\n",
        "start = \"Once upon a time, there is a unicorn\"\n",
        "start_ids = enc.encode(start, bos=True, eos=False)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(enc.decode(y[0].tolist()))\n",
        "            print('---------------')"
      ],
      "metadata": {
        "id": "HMyYgq7f79tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ** check if the same devices\n",
        "\n",
        "print(\"Device of model parameters:\", next(model.parameters()).device)\n",
        "print(\"Device of input tensor 'x':\", x.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-A7TCT6x0hR",
        "outputId": "b9ff5189-f982-401c-9509-c4908e50d33f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".  ..  .config\t.ipynb_checkpoints  sample_data  Stat-8931-GenAI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkAK648esY0H",
        "outputId": "381cbcaf-138d-47ec-adac-da4947e0e0f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "drwxr-xr-x 1 root root 4096 Sep 10 13:28 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('https://drive.google.com/file/d/1SU7jSZI36KGwBv5-zgc3WkStPK6lGKwL/view', 'r') as _:\n",
        "  _"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "U-LnjCDDsBhE",
        "outputId": "157ef9b3-87cb-4b1f-faa1-135894329a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'https://drive.google.com/file/d/1SU7jSZI36KGwBv5-zgc3WkStPK6lGKwL/view'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-560a04d6a60b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://drive.google.com/file/d/1SU7jSZI36KGwBv5-zgc3WkStPK6lGKwL/view'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0m_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https://drive.google.com/file/d/1SU7jSZI36KGwBv5-zgc3WkStPK6lGKwL/view'"
          ]
        }
      ]
    }
  ]
}