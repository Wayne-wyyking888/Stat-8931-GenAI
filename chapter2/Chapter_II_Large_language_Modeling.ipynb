{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNfFyehNdekQefkuB3Of6kg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wayne-wyyking888/Stat-8931-GenAI/blob/main/chapter2/Chapter_II_Large_language_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages, Paths & Environment"
      ],
      "metadata": {
        "id": "Fxm6kceGpwQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1). Download the dependent `.py` files; trained model; tokenizer to the **currect directory**"
      ],
      "metadata": {
        "id": "HaIDtMBLtWy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download model.py and tokenizer.py to the default directory\n",
        "! gdown --id 1SU7jSZI36KGwBv5-zgc3WkStPK6lGKwL -O /content/model.py # download model.py\n",
        "! gdown --id 1uXCgdmip79J6efM5hiHGCy9mdr_U8BXT -O /content/tokenizer.py # download tokenizer.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvVP5B0x23TJ",
        "outputId": "63d9f558-34ae-4c7e-94c5-3aee2e52f20c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1SU7jSZI36KGwBv5-zgc3WkStPK6lGKwL\n",
            "From (redirected): https://drive.google.com/uc?id=1SU7jSZI36KGwBv5-zgc3WkStPK6lGKwL&confirm=t&uuid=313387d4-acac-44a0-8578-78151e0c8a10\n",
            "To: /content/model.py\n",
            "100% 13.3k/13.3k [00:00<00:00, 34.0MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1uXCgdmip79J6efM5hiHGCy9mdr_U8BXT\n",
            "From (redirected): https://drive.google.com/uc?id=1uXCgdmip79J6efM5hiHGCy9mdr_U8BXT&confirm=t&uuid=c435cc11-712b-436e-9cc2-9cac99886211\n",
            "To: /content/tokenizer.py\n",
            "100% 1.35k/1.35k [00:00<00:00, 5.01MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qHiS9pHfpS4V"
      },
      "outputs": [],
      "source": [
        "# trained language model (checkpoints) and tokenizer files under suitable directories (download and unzip files first !!)\n",
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "def download_and_unzip(file_id, output_dir=None):\n",
        "\n",
        "    if output_dir is None:\n",
        "        output_dir = os.getcwd()\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Download file\n",
        "    url = f'https://drive.google.com/uc?id={file_id}'\n",
        "    output = os.path.join(output_dir, 'temp.zip')\n",
        "    gdown.download(url, output, quiet=False)\n",
        "\n",
        "    # Unzip file\n",
        "    with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "        # Get the name of the first file in the archive\n",
        "        original_name = zip_ref.namelist()[0]\n",
        "        zip_ref.extractall(output_dir)\n",
        "\n",
        "    # Remove the temporary zip file\n",
        "    os.remove(output)\n",
        "\n",
        "    # The path to the extracted file\n",
        "    extracted_file = os.path.join(output_dir, original_name)\n",
        "\n",
        "    print(f\"File extracted as: {extracted_file}, saved to {output_dir}\")\n",
        "    return extracted_file\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create sub-directory under models/ and data/\n",
        "checkpoint = download_and_unzip('1bJMOyA86CDayzwmU5KjlZnbhCXHUzO41', output_dir = os.getcwd() + \"/models/trained_model.pt\")\n",
        "tokenizer = download_and_unzip('1UhsXL-ymGFy1fBftMvMbss2PGFzRxZV4', output_dir = os.getcwd() + \"/data/trained_tokenizer.model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc-v3fXlC5zq",
        "outputId": "dd4773aa-96bf-453e-dcb9-f7521f363c42"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1bJMOyA86CDayzwmU5KjlZnbhCXHUzO41\n",
            "From (redirected): https://drive.google.com/uc?id=1bJMOyA86CDayzwmU5KjlZnbhCXHUzO41&confirm=t&uuid=d154d7ed-cbd2-4633-bab3-255f02a7bad4\n",
            "To: /content/models/trained_model.pt/temp.zip\n",
            "100%|██████████| 182M/182M [00:01<00:00, 96.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File extracted as: /content/models/trained_model.pt/trained_model_tok32000.pt, saved to /content/models/trained_model.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UhsXL-ymGFy1fBftMvMbss2PGFzRxZV4\n",
            "To: /content/data/trained_tokenizer.model/temp.zip\n",
            "100%|██████████| 500k/500k [00:00<00:00, 74.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File extracted as: /content/data/trained_tokenizer.model/tok32000.model, saved to /content/data/trained_tokenizer.model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2) Load pretrained model and test\n",
        "* Load pretrained model and pretrained tokenizer\n",
        "* Adjust TF32 precision\n",
        "* Config parameters for generation & decoding"
      ],
      "metadata": {
        "id": "RaX8l8O36AiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from contextlib import nullcontext\n",
        "import torch\n",
        "from model import ModelArgs, Transformer\n",
        "from tokenizer import Tokenizer\n",
        "import os\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Use device: {device}\")\n",
        "\n",
        "# load checkpoint\n",
        "checkpoint_dict = torch.load(checkpoint, map_location=device)\n",
        "gptconf = ModelArgs(**checkpoint_dict['model_args'])\n",
        "model = Transformer(gptconf)\n",
        "state_dict = checkpoint_dict['model']\n",
        "unwanted_prefix = '_orig_mod.' #the unwanted prefix was sometimes added during compiling\n",
        "for k,v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# load tokenizer\n",
        "enc = Tokenizer(tokenizer_model=tokenizer)\n",
        "\n",
        "# adjust precision\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "torch.backends.cuda.matmul.allow_tf32 = True #enables the use of TF32 for matrix multiplication operations within PyTorch when using CUDA\n",
        "torch.backends.cudnn.allow_tf32 = True #enables the use of TF32 precision within the cuDNN library\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fJzX3XSyAP3",
        "outputId": "4d90dba6-9319-44d2-891f-f11c0dd08dca"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-ec5a4e0f16ce>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint_dict = torch.load(checkpoint, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#words: 32000 - BOS ID: 1 - EOS ID: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Generate sample texts**"
      ],
      "metadata": {
        "id": "T8S-jJxPGwxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## parameter configurations\n",
        "num_samples = 3 # number of samples to draw (how many paragraphs?)\n",
        "max_new_tokens = 1024 # number of tokens generated in each sample\n",
        "temperature = 1.0 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 300 # retain only the top_k most likely tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "8i806WDT2orY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate texts\n",
        "start = \"Once upon a time, there is a beautiful 27-year-old lady called ?, \"\n",
        "start_ids = enc.encode(start, bos=True, eos=False)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(enc.decode(y[0].tolist()))\n",
        "            print('---------------')"
      ],
      "metadata": {
        "id": "HMyYgq7f79tS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7af083b-cf04-4984-c980-5ebe12f19673"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, there is a beautiful 27-year-old lady called zzxxlll,  year old. Her name was Emo, and full of joy.\n",
            "Little Olce searched and searched at the open space, and was filled with wonder. She eventually saw the huge structure, and was filled with presents.\n",
            "Celffy saw how peaceful it felt and how the open area around it was full of laughter and joy. Little Eoo quickly filled his mouth with glee.\n",
            "When T-Eack was full, she was filled with love and relax. She spent the days asking her friends and family to come and go to the same place.\n",
            "The two of them spent many hours playing on the wide structure, roasting marcoanks and doing as they told stories. Every once in a while, she and her friends would have a soft, lively time, enjoying their time together.\n",
            "At the end of the day, Todely Elephant asked Y Star how she was such a wonderful group of friends.\n",
            "\"I made it free from this color of metal,\" said Y syruply, smiling. \n",
            "Yve-Craagers were so pleased. They thanked Yrandy for making their day so special, and Yagonco was happy that he had made his own special. One day, a busy bee named Bob was flying around. He saw a twisty path in the water. Bob wanted to see what was down there. So, he turned left and right.\n",
            "\"Look, Bird!\" said Bob. \"I can go down the twisty path. It is fun!\" But, oh no! Bob could not go step by step. He felt sad and wanted to go back.\n",
            "Bob saw a big rock. He thought, \"I can turtle up here.\" But, there were no holes on the path. Bob kept going and didn't give up. He was too busy asking the rock on the twisty path.\n",
            "Finally, Bob asked a frog \"Can I go down the twisty path with me?\" The frog said, \"Yes, but be careful and not fall.\" So, Bob climbed the rock and went on the twisty path. He was very happy and thanked the frog for helping him. Once upon a time, there was a little ant named Annie. Annie was carrying a big piece of earth to her home. She was very proud of her earth very much.\n",
            "One day, when Annie was carrying her earth, she met her friend Timmy the ant. Timmy was tired and hungry, so Annie asked him if he wanted to help her carry the earth. Timmy was ignorant about how important it was to carry earth.\n",
            "Annie showed Timmy how to pick up the earth around its right aid. However, Timmy was not strong enough to lift the earth. He realized that Annie was loyal to him and that it was important to be responsible with the earth. \n",
            "From that day on, Timmy learned to be more responsible and responsible. He said, \"Doing earth is great to help others, but it's also important to be responsible. That's what makes you a good friend.\" And Annie was proud of herself too. Once upon a time, there was a little girl named Lily. She liked to dance and twirl around her room. One day, she saw a monster in her room! The monster was big and green with eight arms. Lily was scared, but she was also brave. She picked up a lollipop and put it in her mouth. The monster didn't like the lollipop at first, but then they both started to dance together. They became friends and danced together every day. Lily wasn't scared anymore when she saw the monster because she knew it was nice to have company. Once upon a time, there was a big store. In the store, there was a staff who helped people. The staff was always available to help people.\n",
            "One day, a little boy came to the store. He saw the staff and asked, \"Can you take me to my mom?\" The staff wanted to help the boy. So, they went on a walk to find his mom.\n",
            "They looked and looked, but they could not find her. Then, they saw a big monster. The monster took the staff far away. The boy and the staff were sad. But, the monster did not take well. It took a big train and a small train. The boy and the staff were very sad. Once upon a time, there was a big, tough dog named Max. Max loved to nap all day long. One day, while Max was sleeping near the window, he heard a little noise outside. He woke up and went to the window to see what it was.\n",
            "Outside, Max found a little bird. The\n",
            "---------------\n",
            "Once upon a time, there is a beautiful 27-year-old lady called zzxxlll, \n",
            "\"I want to go to the park,\" she said, pointing at the park.\n",
            "So zigzag minutes ran around the playground, zigzagging tall trees and hopping through the grass. \n",
            "Soon, it was time for lunch. \n",
            "\"Bye zigzag, zigzag!\" they said out loud.\n",
            "They went running as fast as they could to the park and enjoyed it under the warm sunshine, giggling with delight.\n",
            "But when they arrived, they noticed two cats sticking together in the grass and zigzagging on the ground. \n",
            "\"Oh no,\" said the last girl. \"Let's chase them away!\"\n",
            "So, the girl and her zigzag zigzag chased the fierce cat, chasing the cats and even the birds!\n",
            "Just as they were about to jump, they heard a loud snap. They had run off into the nearby woods and were no longer in the park.\n",
            "The little girl and her zigzag zigzag sad. She knew someone had come across. But she wasn't begging; she couldn't come back home without her. Once upon a time, there was a little girl named Lily. She had a cube toy that she loved to play with. It was her favorite toy in the whole wide world. One day, Lily went to the park to play with her cube toy. She met a boy named Timmy who was holding a tiny ball. \n",
            "\"Hi there, do you want to play with me and my ball?\" asked Lily.\n",
            "\"Yes, please!\" replied Timmy.\n",
            "Lily and Timmy played together for hours, making noise and having fun. When it was time to go home, they hugged and said goodbye. Lily was so happy she made a new friend and couldn't wait to come back to the park to see Timmy again. Once upon a time, there was a little girl named Lily. One day, her mom gave her a journal. Lily loved to write and draw in her journal. \n",
            "One day, austless wind blew the journal away. Lily and her mom searched and searched but couldn't find it. Lily was sad because she loved her journal so much. \n",
            "Lily decided to draw the journal, but she didn't have any stickers left. She asked her mom for help. Her mom got some stickers and wrote, \"Please find your journal, Lily. Here's a stick to make it stick again.\" \n",
            "Lily was happy and decided to draw in her journal. She drew a picture of the park, a sunflower, and a big tree. Even though her journal was gone, she knew she could always draw it again. Once upon a time, a little girl named Lily woke up today. It was very dark outside, and she felt scared. She asked her mom, \"Mommy, can I go outside and play?\" \n",
            "Her mom said, \"Yes, but be careful. Don't wander too far until it's dark outside.\" \n",
            "Lily put on her warm shoes and went outside. She saw that the streets were full of people, so she started to walk. She said to herself, \"I can't stop, it's too dark.\" \n",
            "Just then, a police officer came by and said, \"Can you help me? I closed the mailbox and now the houses will be free to move away.\" \n",
            "Lily said, \"I can give you a lollipop if you want.\" The police officer was so happy, and they entered the neighborhood together. The moral of the story is that even when scary things happen, there are always people who can help you feel better and less scary. One day, a little girl named Mia went for a walk. She saw a big box. The box was closed. Mia was sad. She wanted to see what was inside the box.\n",
            "Mia tried to open the box with her hands. But the box was hard to open. She looked for help. Mia found her friend Tom. Tom was very strong. Tom came over and helped Mia open the box.\n",
            "Inside the big box, they found many toys and games. Mia and Tom were very happy. They played with the toys and had lots of fun. Mia learned that helping others is a good thing to do. Once upon a time, in a lovely little house, lived a happy cat named Mimi. Mimi had a friend, a small mouse named Tim. Tim was always safe and happy. They liked to play together.\n",
            "One day, Mimi and Tim found a big box. \"Let's unpack it\n",
            "---------------\n",
            "Once upon a time, there is a beautiful 27-year-old lady called zzxxlll,  Elephant! Everyday she would stare out of her home and try to find a very important job in the city.\n",
            "One day, she decided to go in the busy car down the hallway. She bent down and laid her ticket in her lap. As she did in the car, she spotted a manbird in the driveway. The manbird was carrying a basket full of eggs. Zigels were so excited to see them! \n",
            "Zig numbers were very important for the babybird. She smiled at the manbird and said “Thank you!” Her zigzag path led her very far away from the busy city, so she could go again whenever she wanted someone to rely on. She waddled around the city, looking for a place to explore. \n",
            "Just then, a little boy walked in. He said hello to Zigar bird, who had seenraarags walking in traffic and had been playing with a ball in the busy city. Zigaran waved back and showed the boy what her walk was. From that day on, the little boy and Zigornbird started to walk together, both wearing their warm hats and measuring weights in the city. They were care having lots of fun soaring together. Once upon a time, there was a little girl named Lily. She loved to play outside in the sun with her friends. One day, Lily and her friends were playing tag when they saw a cord hanging from a tree. Lily's friend, Timmy, said \"Let's play with the cord like swinging!\" \n",
            "Lily's other friend, Sarah, noticed the cord and said \"I think it's too small for us.\" But Timmy didn't know what to do. He was ignorant about the cord. Lily suggested they play with the other players instead. \n",
            "Sarah thought that was funny and said \"That's a great idea, let's do it!\" So they all played together and had a great time. From that day on, the cord became a sturdy rope for all of them to play with and take turns pretending. One day, a red motor made a big mess in the store. It was filthy. People could not use it. They were sad.\n",
            "A man saw the motor and said, \"We can mix up the clean water to make it clean again!\" Tim and his mom were happy. They helped to clean the car. The man painted it green and it became clear.\n",
            "Now, when the train engine rained, the dirt would spring into the sky and make the car clean. Everyone cheered, \"Yay!\" The cars looked like new again. They all lived happily ever after. Once upon a time, there was a king. The king was very fierce and wore a big crown on his head. He lived in a big castle with many staff. The staff were very kind and took good care of the king.\n",
            "One day, the king met a little boy in his castle. The boy wanted to fight the king. He said, \"Please, let me be your soldier. I want to help you fight. I know a big secret place that takes all the time.\"\n",
            "The king laughed and said, \"You are too small to fight with me. But if you win, I will say hello and tell you many stories.\" The boy listened to the king and felt happy.\n",
            "A brave knight came to the castle and saw the little boy. The knight said to the boy, \"Let us fight!\" The boy was scared and said, \"No, we must fight to find the treasure.\" The king agreed and let the boy keep running.\n",
            "The boy finally got to the home and the king smiled. The king said, \"Thank you, brave knight. You did the right thing. Now you can go back to your castle and you have been defeated.\" The boy went back to his castle and shared his treasure with the king. They were both happy and had a great day. Once upon a time, there was a little girl named Lily. She had a big clock in her room that tick-tocked all day long. One day, Lily's mom brought a yummy snack for her to eat. But Lily didn't want to take a bite, she just wanted to eat her favorite chocolate. \n",
            "As Lily was eating her snack, her little brother came into the room. He was very rude to Lily and started to be loud. \"Get out of my snack!\" shouted Lily. But her brother didn't listen and kept making a big mess. Lily's mom came in and saw what was happening. \n",
            "She talked to Lily's brother about being nicer and giving up. With her m\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Glance at Decoding from A Trained Model"
      ],
      "metadata": {
        "id": "fVMGs6YiJIkj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1). The `generate()` function takes auto-regression procedures"
      ],
      "metadata": {
        "id": "5Mt68544JWmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `generate` is a method under the model class. It takes a conditioning sequence of indices `idx`, which is a LongTensor of shape `(batch_size, sentence_length)`, and completes the sequence `max_new_tokens` times, feeding the predictions back into the model each time. The function is often operated under `model.eval()` mode.\n",
        "* **AR (auto-regression) generation** **by considering the next token sampled from a softmax layer**\n",
        "* `@torch.inference_mode()` is an alternative to `with torch.no_grad()` to ***disable gradient calculation !***"
      ],
      "metadata": {
        "id": "-5SQyKT9KIoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()\n",
        "def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx if idx.size(1) <= self.params.max_seq_len else idx[:, -self.params.max_seq_len:]\n",
        "        logits = self(idx_cond)\n",
        "        logits = logits[:, -1, :] # crop to just the final time step\n",
        "        if temperature == 0.0:\n",
        "            _, idx_next = torch.topk(logits, k=1, dim=-1)\n",
        "        else:\n",
        "            logits = logits / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "xkAK648esY0H"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **To customize a new generation approach"
      ],
      "metadata": {
        "id": "EsLn7LZ0Lybq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "    # Custom generation logic here\n",
        "    for _ in range(max_new_tokens):\n",
        "        ...\n",
        "    return idx\n",
        "\n",
        "# Assign the custom generate method to the model instance\n",
        "model.generate = custom_generate.__get__(model, Transformer)"
      ],
      "metadata": {
        "id": "cxexPdFjL7-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2) A closer look at `generate()` function component"
      ],
      "metadata": {
        "id": "zIRklasAOor8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Greedy decoding** : Always pick the next word *with the highest probability*. This method can lead to repetitive and predictable text. `temperature = 0.0`\n",
        "\n",
        "```\n",
        "if temperature == 0.0:\n",
        "    _, idx_next = torch.topk(logits, k=1, dim=-1)\n",
        "    idx = torch.cat((idx, idx_next), dim=1)\n",
        "```"
      ],
      "metadata": {
        "id": "_Oj2DvmmMs15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Temperature Scaling** :  A lower temperature makes the distribution peakier **(more greedy)**, while a higher temperature makes the distribution flatter **(more random)**.\n",
        "\n",
        "```\n",
        "logits = logits / temperature\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "idx_next = torch.multinomial(probs, num_samples=1)\n",
        "idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "8o56tDtgNdT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Top-k Sampling**: Limits the next word choices to the top k most probable words ($p_1 \\ge p_2 \\ge ... \\ge p_k$):\n",
        "\n",
        "```\n",
        "if top_k is not None:\n",
        "    values, indices = torch.topk(logits, k=top_k)\n",
        "    logits[logits < values[:, [-1]]] = -float('Inf')\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    idx_next = torch.multinomial(probs, num_samples=1)\n",
        "    idx = torch.cat((idx, idx_next), dim=1)\n",
        "```\n",
        "4. **Top-p (Nucleus) Sampling**: This approach chooses the **smallest set** of words whose cumulative probability exceeds the probability $p$, a threshold.\n",
        "\n",
        "```\n",
        "cum_probs = torch.cumsum(F.softmax(logits, dim=-1), dim=-1)\n",
        "threshold = torch.rand(1).item()\n",
        "idx_next = torch.min((cum_probs > threshold).nonzero(as_tuple=True)[1])\n",
        "idx = torch.cat((idx, idx_next.unsqueeze(0)), dim=1)\n",
        "```"
      ],
      "metadata": {
        "id": "vZjeedE9PO54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Beam-search decoding** : Beam search *maintains multiple hypotheses* (the “beam”) at each step and expands them further by exploring several possible next steps. This strategy balances between *breadth (diversity) and depth (accuracy)*.\n",
        "\n",
        "```\n",
        "beam_width = 5\n",
        "candidates = [idx]\n",
        "for _ in range(max_new_tokens):\n",
        "    all_candidates = []\n",
        "    for candidate in candidates:\n",
        "        logits = self(candidate)\n",
        "        probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "        top_probs, top_idx = torch.topk(probs, k=beam_width)\n",
        "        for i in range(beam_width):\n",
        "            next_candidate = torch.cat((candidate, top_idx[:, i:i+1]), dim=1)\n",
        "            all_candidates.append((next_candidate, top_probs[:, i].item()))\n",
        "    ordered = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n",
        "    candidates = [x[0] for x in ordered[:beam_width]]\n",
        "idx = candidates[0]\n",
        "```"
      ],
      "metadata": {
        "id": "B_w8sVtlRT4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization and Vocabulary"
      ],
      "metadata": {
        "id": "dYP3wuABRk2w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YbA7j-l4POiI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}