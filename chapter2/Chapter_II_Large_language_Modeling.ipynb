{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMVXX3XuQ2ugNigECHlmRRt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wayne-wyyking888/Stat-8931-GenAI/blob/main/chapter2/Chapter_II_Large_language_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages, Paths & Environment"
      ],
      "metadata": {
        "id": "Fxm6kceGpwQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1). Download the dependent `.py` files; trained model; tokenizer to the **currect directory**"
      ],
      "metadata": {
        "id": "HaIDtMBLtWy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download model.py and tokenizer.py to the default directory\n",
        "! gdown --id 1SU7jSZI36KGwBv5-zgc3WkStPK6lGKwL -O /content/model.py # download model.py\n",
        "! gdown --id 1uXCgdmip79J6efM5hiHGCy9mdr_U8BXT -O /content/tokenizer.py # download tokenizer.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvVP5B0x23TJ",
        "outputId": "2219de99-4a91-4845-cf3b-c69adaa965fc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1SU7jSZI36KGwBv5-zgc3WkStPK6lGKwL\n",
            "From (redirected): https://drive.google.com/uc?id=1SU7jSZI36KGwBv5-zgc3WkStPK6lGKwL&confirm=t&uuid=060aabd3-a1f3-46fc-b5b7-2fe3c6e1335b\n",
            "To: /content/model.py\n",
            "100% 13.3k/13.3k [00:00<00:00, 26.1MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1uXCgdmip79J6efM5hiHGCy9mdr_U8BXT\n",
            "From (redirected): https://drive.google.com/uc?id=1uXCgdmip79J6efM5hiHGCy9mdr_U8BXT&confirm=t&uuid=cd7c0bae-e848-48db-9eb4-b8dc4389d705\n",
            "To: /content/tokenizer.py\n",
            "100% 1.35k/1.35k [00:00<00:00, 4.09MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qHiS9pHfpS4V"
      },
      "outputs": [],
      "source": [
        "# trained language model (checkpoints) and tokenizer files under suitable directories (download and unzip files first !!)\n",
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "def download_and_unzip(file_id, output_dir=None):\n",
        "\n",
        "    if output_dir is None:\n",
        "        output_dir = os.getcwd()\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Download file\n",
        "    url = f'https://drive.google.com/uc?id={file_id}'\n",
        "    output = os.path.join(output_dir, 'temp.zip')\n",
        "    gdown.download(url, output, quiet=False)\n",
        "\n",
        "    # Unzip file\n",
        "    with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "        # Get the name of the first file in the archive\n",
        "        original_name = zip_ref.namelist()[0]\n",
        "        zip_ref.extractall(output_dir)\n",
        "\n",
        "    # Remove the temporary zip file\n",
        "    os.remove(output)\n",
        "\n",
        "    # The path to the extracted file\n",
        "    extracted_file = os.path.join(output_dir, original_name)\n",
        "\n",
        "    print(f\"File extracted as: {extracted_file}, saved to {output_dir}\")\n",
        "    return extracted_file\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create sub-directory under models/ and data/\n",
        "checkpoint = download_and_unzip('1bJMOyA86CDayzwmU5KjlZnbhCXHUzO41', output_dir = os.getcwd() + \"/models/trained_model.pt\")\n",
        "tokenizer = download_and_unzip('1UhsXL-ymGFy1fBftMvMbss2PGFzRxZV4', output_dir = os.getcwd() + \"/data/trained_tokenizer.model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc-v3fXlC5zq",
        "outputId": "442a91aa-4e69-4047-e4c6-5f8431fcbecb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1bJMOyA86CDayzwmU5KjlZnbhCXHUzO41\n",
            "From (redirected): https://drive.google.com/uc?id=1bJMOyA86CDayzwmU5KjlZnbhCXHUzO41&confirm=t&uuid=1e6edaa9-2d4b-4b87-8d44-04b971c25072\n",
            "To: /content/models/trained_model.pt/temp.zip\n",
            "100%|██████████| 182M/182M [00:02<00:00, 66.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File extracted as: /content/models/trained_model.pt/trained_model_tok32000.pt, saved to /content/models/trained_model.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UhsXL-ymGFy1fBftMvMbss2PGFzRxZV4\n",
            "To: /content/data/trained_tokenizer.model/temp.zip\n",
            "100%|██████████| 500k/500k [00:00<00:00, 71.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File extracted as: /content/data/trained_tokenizer.model/tok32000.model, saved to /content/data/trained_tokenizer.model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2) Load pretrained model and test\n",
        "* Load pretrained model and pretrained tokenizer\n",
        "* Adjust TF32 precision\n",
        "* Config parameters for generation & decoding"
      ],
      "metadata": {
        "id": "RaX8l8O36AiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from contextlib import nullcontext\n",
        "import torch\n",
        "from model import ModelArgs, Transformer\n",
        "from tokenizer import Tokenizer\n",
        "import os\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Use device: {device}\")\n",
        "\n",
        "# load checkpoint\n",
        "checkpoint_dict = torch.load(checkpoint, map_location=device)\n",
        "gptconf = ModelArgs(**checkpoint_dict['model_args'])\n",
        "model = Transformer(gptconf)\n",
        "state_dict = checkpoint_dict['model']\n",
        "unwanted_prefix = '_orig_mod.' #the unwanted prefix was sometimes added during compiling\n",
        "for k,v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# load tokenizer\n",
        "enc = Tokenizer(tokenizer_model=tokenizer)\n",
        "\n",
        "# adjust precision\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "torch.backends.cuda.matmul.allow_tf32 = True #enables the use of TF32 for matrix multiplication operations within PyTorch when using CUDA\n",
        "torch.backends.cudnn.allow_tf32 = True #enables the use of TF32 precision within the cuDNN library\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fJzX3XSyAP3",
        "outputId": "8987bff3-9d8d-4e9e-f941-5a41a7f4d70c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-ec5a4e0f16ce>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint_dict = torch.load(checkpoint, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#words: 32000 - BOS ID: 1 - EOS ID: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Generate sample texts**"
      ],
      "metadata": {
        "id": "T8S-jJxPGwxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## parameter configurations\n",
        "num_samples = 1 # number of samples to draw (how many paragraphs?)\n",
        "max_new_tokens = 1024 # number of tokens generated in each sample\n",
        "temperature = 1.2 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 300 # retain only the top_k most likely tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "8i806WDT2orY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate texts\n",
        "start = \"your model is so trash! Do you know?\"\n",
        "start_ids = enc.encode(start, bos=True, eos=False)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(enc.decode(y[0].tolist()))\n",
        "            print('---------------')"
      ],
      "metadata": {
        "id": "HMyYgq7f79tS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "742477d5-4d1d-455b-bc7c-6e1a96a74a92"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your model is so trash! Do you know? Does he win for sure?” \n",
            "A lady came over to hear the news. \"No, he may win,\" the lady said with a smile. \n",
            "Little did the lady think that saving her make a lot of money. She did her special job to protect the trash, caring for anyone it makes. Every day, she would and try to help with anything she dared. flowers bloom in the garden and made the neighborhood applauding. \n",
            "Many people also offered politely compliment the lady. They said things like she would never pick until the flowers were picked up. Luckily, they never had to refuse the man's decision.  Everyone worked together in a unique way – labeled this was exactly what they wanted and let it stay. \n",
            "The town was successful and organized once again! \n",
            "The lady thanked all her helpers and decided to make even more brilliant math. Happy — and busy love for her love. People always admired her magic of building the paper no more. Once upon a time, there was a little squirrel named Sam. Sam had a nut that he loved to carry all around the proud tree. One day, Sam looked up at the sky and he knew it would be dangerous to bring up his nut in the same spot as the one he had left where he last collected it. \n",
            "Sam quickly realised that it was a mountain with fire. Right before he could make his nut shine bright grey as steam as staracles. The leaves huddled together, with all the sparkles of the large brown and black dust spread all around him.\n",
            "Sam quickly ran over and said, \"Thank you!\" Even the animals at the rock he provided him was sweaty from the coal. But suddenly, Sam heard a loud and rumbling noise from above. In thunderunder, Sam was fearful and ran out of the camp for the rest of the day. \n",
            "From then on Sam stayed safe and warm under under his nut tree's leaves. Whenever people needed to play, Sam and his nut shone bright as they planted Rosies around while it plunged in. Once upon a time, there was a boy named Timmy. Timmy had a pocket on his dress that he liked to put things in. One day, Timmy went to the park to play with his friends. They played on the swings and the slide, but Timmy wanted to get his pocket toy from the tree. He climbed up the tree and got it. \n",
            "When he got home, he put his shiny toy in his pocket! But when his charming shirt got dirty, he didn't want to wear it anymore. He went to play without it because he didn't want to listen to this important earbrub in his pocket. His mom had helped him off the stain and set the shirt away. Timmy learned an important lesson that day. Even if something seemed so special or dangerous, it's important to listen to your ears while you save a life better. Once upon a time, there was a big bird called an ostrich. The ostrich lived in a big field with lots of leaves. One day, a foolish man came to the field. He wanted to eat the big bird. The parrot saw the rude bird and got scared. \n",
            "The parrot flew away very fast. The big bird followed him. The bulades were black and made smoke. The people who worked there didn't want the squirrels to come near. They asked the seeds to bring water, but the naughty parrot refused. Soon, the people found the big bird and left. The seeds were safe and sound. The end. Once upon a time, there was a dog. The dog was gray and cute. A girl named Mia loved the dog very much. Mia owned a toy horse that seemed brand new. She loved playing with the horse every night before bed. \n",
            "One night, the dog went to see that it had a razor that belonged to Mommy. Mia saw the razor and thought it was useful. She pretended to shave the dog like the horse she met the day before. The dog watched what she was doing with the razor, and then it wagged its tail. \n",
            "A few days later, Mia went back to the park to pet the dog and hear its same whip. When she got close, the pale brown horse he passed by. Mia thought it was so cute with the roues eyes and tail. But then, the big gray horse struck the bicycle and it fell again! Mia was surprised, but still loved the horse and stayed her pet banging with it in the tide. From that day on, Mia and her gray horse the dog\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Glance at Decoding from A Trained Model"
      ],
      "metadata": {
        "id": "fVMGs6YiJIkj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1). The `generate()` function takes auto-regression procedures"
      ],
      "metadata": {
        "id": "5Mt68544JWmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `generate` is a method under the model class. It takes a conditioning sequence of indices `idx`, which is a LongTensor of shape `(batch_size, sentence_length)`, and completes the sequence `max_new_tokens` times, feeding the predictions back into the model each time. The function is often operated under `model.eval()` mode.\n",
        "* **AR (auto-regression) generation** **by considering the next token sampled from a softmax layer**\n",
        "* `@torch.inference_mode()` is an alternative to `with torch.no_grad()` to ***disable gradient calculation !***"
      ],
      "metadata": {
        "id": "-5SQyKT9KIoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()\n",
        "def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx if idx.size(1) <= self.params.max_seq_len else idx[:, -self.params.max_seq_len:]\n",
        "        logits = self(idx_cond)\n",
        "        logits = logits[:, -1, :] # crop to just the final time step\n",
        "        if temperature == 0.0:\n",
        "            _, idx_next = torch.topk(logits, k=1, dim=-1)\n",
        "        else:\n",
        "            logits = logits / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "xkAK648esY0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **To customize a new generation approach"
      ],
      "metadata": {
        "id": "EsLn7LZ0Lybq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "    # Custom generation logic here\n",
        "    for _ in range(max_new_tokens):\n",
        "        ...\n",
        "    return idx\n",
        "\n",
        "# Assign the custom generate method to the model instance\n",
        "model.generate = custom_generate.__get__(model, Transformer)"
      ],
      "metadata": {
        "id": "cxexPdFjL7-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2) A closer look at `generate()` function component"
      ],
      "metadata": {
        "id": "zIRklasAOor8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Greedy decoding** : Always pick the next word *with the highest probability*. This method can lead to repetitive and predictable text. `temperature = 0.0`\n",
        "\n",
        "```\n",
        "if temperature == 0.0:\n",
        "    _, idx_next = torch.topk(logits, k=1, dim=-1)\n",
        "    idx = torch.cat((idx, idx_next), dim=1)\n",
        "```"
      ],
      "metadata": {
        "id": "_Oj2DvmmMs15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Temperature Scaling** :  A lower temperature makes the distribution peakier **(more greedy)**, while a higher temperature makes the distribution flatter **(more random)**.\n",
        "\n",
        "```\n",
        "logits = logits / temperature\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "idx_next = torch.multinomial(probs, num_samples=1)\n",
        "idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "8o56tDtgNdT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Top-k Sampling**: Limits the next word choices to the top k most probable words ($p_1 \\ge p_2 \\ge ... \\ge p_k$):\n",
        "\n",
        "```\n",
        "if top_k is not None:\n",
        "    values, indices = torch.topk(logits, k=top_k)\n",
        "    logits[logits < values[:, [-1]]] = -float('Inf')\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    idx_next = torch.multinomial(probs, num_samples=1)\n",
        "    idx = torch.cat((idx, idx_next), dim=1)\n",
        "```\n",
        "4. **Top-p (Nucleus) Sampling**: This approach chooses the **smallest set** of words whose cumulative probability exceeds the probability $p$, a threshold.\n",
        "\n",
        "```\n",
        "cum_probs = torch.cumsum(F.softmax(logits, dim=-1), dim=-1)\n",
        "threshold = torch.rand(1).item()\n",
        "idx_next = torch.min((cum_probs > threshold).nonzero(as_tuple=True)[1])\n",
        "idx = torch.cat((idx, idx_next.unsqueeze(0)), dim=1)\n",
        "```"
      ],
      "metadata": {
        "id": "vZjeedE9PO54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Beam-search decoding** : Beam search *maintains multiple hypotheses* (the “beam”) at each step and expands them further by exploring several possible next steps. This strategy balances between *breadth (diversity) and depth (accuracy)*.\n",
        "\n",
        "```\n",
        "beam_width = 5\n",
        "candidates = [idx]\n",
        "for _ in range(max_new_tokens):\n",
        "    all_candidates = []\n",
        "    for candidate in candidates:\n",
        "        logits = self(candidate)\n",
        "        probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "        top_probs, top_idx = torch.topk(probs, k=beam_width)\n",
        "        for i in range(beam_width):\n",
        "            next_candidate = torch.cat((candidate, top_idx[:, i:i+1]), dim=1)\n",
        "            all_candidates.append((next_candidate, top_probs[:, i].item()))\n",
        "    ordered = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n",
        "    candidates = [x[0] for x in ordered[:beam_width]]\n",
        "idx = candidates[0]\n",
        "```"
      ],
      "metadata": {
        "id": "B_w8sVtlRT4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization and Vocabulary"
      ],
      "metadata": {
        "id": "dYP3wuABRk2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vocabulary Construction WITH\n",
        "* **Word2Vec** (static)\n",
        "* **CBOW & Skip Gram**\n",
        "* **Byte Pair Encoding** (BPE): BPE iteratively merges the most frequent pair of bytes or characters in a dataset into a single new token, continuing this process until a specified number of merges (vocabulary size) has been reached"
      ],
      "metadata": {
        "id": "11sBfgSo70_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **\n",
        "-- `SentencePiece` is a package that implements subword tokenization. It is language-independent and can be used to train LLMs **without requiring pre-tokenized text**.\n",
        "\n",
        "```\n",
        "import sentencepiece as spm\n",
        "spm.SentencePieceTrainer.train(input=DATA_PATH, model_prefix=SAVE_PATH, model_type=\"bpe\", vocab_size=VOCAB_SIZE, input_format=\"text\")\n",
        "```"
      ],
      "metadata": {
        "id": "cgH-Z4vg-uQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sentencepiece import SentencePieceProcessor\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, tokenizer_model):\n",
        "        assert os.path.isfile(tokenizer_model), \"Tokenizer model file not found.\"\n",
        "        self.sp_model = SentencePieceProcessor(model_file=tokenizer_model)\n",
        "\n",
        "        # Basic vocabulary properties\n",
        "        self.n_words = self.sp_model.vocab_size()  # Total vocabulary size\n",
        "        self.bos_id = self.sp_model.bos_id()       # Beginning of sentence token\n",
        "        self.eos_id = self.sp_model.eos_id()       # End of sentence token\n",
        "        self.pad_id = self.sp_model.pad_id()       # Padding token\n",
        "\n",
        "    def encode(self, text, bos=True, eos=True):\n",
        "        # Encodes text into a list of token ids\n",
        "        tokens = self.sp_model.encode(text)\n",
        "        if bos:\n",
        "            tokens = [self.bos_id] + tokens\n",
        "        if eos:\n",
        "            tokens += [self.eos_id]\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        # Decodes a list of token ids back into text\n",
        "        return self.sp_model.decode(tokens)\n",
        "\n",
        "# tok = Tokenizer(tokenizer_model)\n",
        "# use tok.encode(text, bos=True, eos=False)\n",
        "# use tok.decode(list_of_integers)"
      ],
      "metadata": {
        "id": "YbA7j-l4POiI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Architecture"
      ],
      "metadata": {
        "id": "UWUjMDftBXZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Transformer(\n",
        "  (tok_embeddings): Embedding(32000, 288) #\n",
        "  (dropout): Dropout(p=0.0, inplace=False)\n",
        "  (layers): ModuleList(\n",
        "    (0-5): 6 x TransformerBlock(\n",
        "      (attention): Attention(\n",
        "        (wq): Linear(in_features=288, out_features=288, bias=False)\n",
        "        (wk): Linear(in_features=288, out_features=288, bias=False)\n",
        "        (wv): Linear(in_features=288, out_features=288, bias=False)\n",
        "        (wo): Linear(in_features=288, out_features=288, bias=False)\n",
        "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
        "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
        "      )\n",
        "      (feed_forward): FeedForward(\n",
        "        (w1): Linear(in_features=288, out_features=768, bias=False)\n",
        "        (w2): Linear(in_features=768, out_features=288, bias=False)\n",
        "        (w3): Linear(in_features=288, out_features=768, bias=False)\n",
        "        (dropout): Dropout(p=0.0, inplace=False)\n",
        "      )\n",
        "      (attention_norm): RMSNorm()\n",
        "      (ffn_norm): RMSNorm()\n",
        "    )\n",
        "  )\n",
        "  (norm): RMSNorm()\n",
        "  (output): Linear(in_features=288, out_features=32000, bias=False)\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "rtBK0rNUBgvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `Embedding(32000, 288)`: Maps input tokens from a vocabulary of **32,000 to 288-dimensional embeddings**.\n",
        "\n",
        "* `Dropout(p=0.0)`: Applies dropout with a probability of 0.0 (effectively no dropout in this setup).\n",
        "\n",
        "* 6 x TransformerBlock: Six layers of Transformer blocks, each containing:\n",
        "\n",
        "**Attention Mechanism**\n",
        "\n",
        "* `Linear(in_features=288, out_features=288)`: Four linear transformations (for queries, keys, values, and output) in the multi-head attention mechanism, all with 288 features.\n",
        "\n",
        "* `Dropout(p=0.0)`: Two dropout layers for attention and residual dropout, both set to 0.0 probability.\n",
        "\n",
        "**FeedForward Network**\n",
        "\n",
        "* `Linear(in_features=288, out_features=768)`: First linear layer of the feed-forward network.\n",
        "\n",
        "* `Linear(in_features=768, out_features=288)`: Second linear layer that projects back to 288 features.\n",
        "\n",
        "* `Dropout(p=0.0)`: Dropout layer in the feed-forward network.\n",
        "\n",
        "**Normalization (LN)**\n",
        "\n",
        "* `RMSNorm()`: Normalization layer for both the output of the attention block and the feed-forward network.\n",
        "\n",
        "**Final Normalization**\n",
        "\n",
        "* `RMSNorm()`: Normalization layer after the last Transformer block.\n",
        "\n",
        "**Output Projection**\n",
        "\n",
        "* `Linear(in_features=288, out_features=32000)`: Linear layer that projects the output of the Transformer to a vocabulary size of 32,000."
      ],
      "metadata": {
        "id": "BbuIdFySBxNl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6UTSu5kKBeiB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}